{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252152f1-16b8-4ada-86e3-c5d6fc600b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문장 데이터\n",
    "sentences = [\n",
    "    '안녕하세요, 반갑습니다.',\n",
    "    '오늘 날씨가 좋네요.',\n",
    "    '내일은 비가 올 것 같아요.',\n",
    "    '저는 공부를 좋아합니다.'\n",
    "    '저는 공부를 좋아합니다.'\n",
    "]\n",
    "\n",
    "# 각 문장에 대한 레이블\n",
    "categories = ['인사', '날씨', '날씨', '취미']\n",
    "\n",
    "\n",
    "# 추가 연습 예제\n",
    "sentences = [\n",
    "    \"오늘 날씨가 좋아서 나들이 가고 싶다.\",\n",
    "    \"이 영화는 정말 재미있었어요.\",\n",
    "    \"맛있는 음식을 먹으러 갈까요?\",\n",
    "    \"운동을 하면 건강에 좋아지는 것 같아요.\",\n",
    "    \"공부하기 싫어서 미루고 있어요.\",\n",
    "    \"여행 계획을 세우고 있는데 어디로 갈까요?\",\n",
    "    \"좋은 책을 읽으면 마음이 편안해져요.\",\n",
    "    \"오늘은 친구들과 만나서 재미있게 놀았어요.\",\n",
    "    \"새로운 언어를 배우는 것은 어려워도 흥미로워요.\",\n",
    "    \"주말에 가족들과 함께 시간을 보내기로 했습니다.\"\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    \"여행/레저\",\n",
    "    \"엔터테인먼트/영화\",\n",
    "    \"식음료/레스토랑\",\n",
    "    \"건강/운동\",\n",
    "    \"교육/학습\",\n",
    "\t\"여행/계획\",\n",
    "\t\"엔터테인먼트/도서\",\n",
    "\t\"인간관계/친구\",\n",
    "\t\"교육/언어학습\",\n",
    "\t\"가족 / 가정\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47440e37-4411-4629-a8ba-fecc281c77da",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ad531d-0f1d-466d-92ce-df85394ce10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 인스턴스 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 리스트 생성 (예시)\n",
    "stopwords = ['가', '고', '을', '를', '이', '는']\n",
    "\n",
    "# 토크나이징 함수 정의\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=stopwords):\n",
    "    return [\n",
    "        word for word, tag in okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅏ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "\n",
    "    ]\n",
    "\n",
    "# Vectorizer 설정 및 fit_transform 실행 \n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer)\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "# type(X), X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ecb98-e242-47d2-bc24-c6d32659e8b8",
   "metadata": {},
   "source": [
    "## LDA 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9859ab9f-40c0-48b4-93f1-a31503e4eda2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93256387 0.86521185 1.03883671 1.02523653 0.80913581 1.07368176\n",
      "  1.0150056  1.13778466 0.92450749 0.88634471 1.05362286 0.9507389\n",
      "  1.02789763 0.91746244 1.15322294 0.98473815 1.01458403 0.97312496\n",
      "  1.02485685 0.83306221 0.8983354  1.04070562 1.15448415 0.94935007\n",
      "  0.81380718 0.83171438 1.05212879 0.94951346 0.774172   1.00437099\n",
      "  1.06190695]\n",
      " [0.87558689 0.97806064 1.1132956  1.00760608 0.92550612 0.97235507\n",
      "  0.95750357 0.91323946 0.89910456 1.05202651 0.96084095 1.04561235\n",
      "  0.91227673 0.95515443 1.13116196 1.02906899 1.02264529 0.97120065\n",
      "  0.87476442 0.91726269 0.82893472 0.92970834 0.8633624  0.95138158\n",
      "  1.09759723 1.05571554 0.99581933 0.92367191 0.93521605 0.90925151\n",
      "  0.94643439]]\n"
     ]
    }
   ],
   "source": [
    "# LDA 모델 설정 및 fit 실행 \n",
    "n_topic=2\n",
    "lda_model=LatentDirichletAllocation(n_components=n_topic, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_top=lda_model.fit(X)\n",
    "print(lda_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f332ce4-c45a-470a-8a46-b0eec35f1914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115.22757889932885, 0.5, 0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.bound_, lda_model.topic_word_prior_, lda_model.doc_topic_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8beb15-b7f7-4320-a808-96891e44cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lda_model.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe607e-1cb1-4aa1-8840-f2afaf919da0",
   "metadata": {},
   "source": [
    "## 각 토픽에 대해 가장 높은 확률을 가진 단어들과 각 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda9a146-2f7b-45c1-abe2-f67a46538495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Topic0  Topic1  dominant_topic\n",
      "Doc0    0.74    0.26               0\n",
      "Doc1    0.63    0.37               0\n",
      "Doc2    0.28    0.72               1\n",
      "Doc3    0.60    0.40               0\n",
      "Doc4    0.69    0.31               0\n",
      "Doc5    0.49    0.51               1\n",
      "Doc6    0.29    0.71               1\n",
      "Doc7    0.72    0.28               0\n",
      "Doc8    0.37    0.63               1\n",
      "Doc9    0.33    0.67               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 각 문장이 어느 토픽에 속하는지 확인\n",
    "topic_output = lda_model.transform(X)\n",
    "\n",
    "# dataframe으로 변환\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(sentences))]\n",
    "df_document_topic = pd.DataFrame(np.round(topic_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# dominant topic 컬럼 생성\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "print(df_document_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5661ff-9194-4aea-b369-bed07a0c267a",
   "metadata": {},
   "source": [
    "## 각 문서에 대해 가장 확률이 높은 토픽과 그 토픽의 상위 단어들을 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d622d1c-1b9c-435c-b61e-e9b157a0f79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Topic0  Topic1  dominant_topic   Topic         TopWords\n",
      "index                                                         \n",
      "Doc0     0.74    0.26               0  Topic0  오늘 보내다 날씨 공부 하다\n",
      "Doc1     0.63    0.37               0  Topic0  오늘 보내다 날씨 공부 하다\n",
      "Doc3     0.60    0.40               0  Topic0  오늘 보내다 날씨 공부 하다\n",
      "Doc4     0.69    0.31               0  Topic0  오늘 보내다 날씨 공부 하다\n",
      "Doc7     0.72    0.28               0  Topic0  오늘 보내다 날씨 공부 하다\n",
      "Doc2     0.28    0.72               1  Topic1  보내다 갈다 음식 읽다 마음\n",
      "Doc5     0.49    0.51               1  Topic1  보내다 갈다 음식 읽다 마음\n",
      "Doc6     0.29    0.71               1  Topic1  보내다 갈다 음식 읽다 마음\n",
      "Doc8     0.37    0.63               1  Topic1  보내다 갈다 음식 읽다 마음\n",
      "Doc9     0.33    0.67               1  Topic1  보내다 갈다 음식 읽다 마음\n"
     ]
    }
   ],
   "source": [
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words[\"Topic\" + str(topic_idx)] = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return top_words\n",
    "\n",
    "# 상위 단어 추출\n",
    "n_top_words = 5\n",
    "top_words = get_top_words(lda_model, vectorizer.get_feature_names(), n_top_words)\n",
    "\n",
    "# DataFrame 생성\n",
    "df_topic_keywords = pd.DataFrame(top_words.items(), columns=['Topic', 'TopWords'])\n",
    "\n",
    "# df_document_topic와 병합\n",
    "df_merged = pd.merge(df_document_topic.reset_index(), df_topic_keywords, left_on='dominant_topic', right_on=df_topic_keywords.index)\n",
    "df_merged.set_index('index', inplace=True)\n",
    "\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fd18a4-974e-466e-b2a8-08b88238fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0:\n",
      "오늘 보내다\n",
      "\n",
      "Topic #1:\n",
      "보내다 갈다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토픽별로 중요도가 높은 단어들을 출력\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# n_top_words = 10 # 상위 10개의 단어를 출력하도록 설정 \n",
    "n_top_words = 2\n",
    "print_top_words(lda_model, vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2293e75-0961-43be-b2c4-ce431cbc147d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
